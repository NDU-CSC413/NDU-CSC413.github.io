---
header-includes:
  - \usepackage{algorithm2e}
---

# CUDA Part 1


CUDA is a parallel programming platform created by NVIDIA. Obviously it works on NVIDIA GPUs.
The massive number of cores on the GPU allows us to create a massive number of concurrent threads. 
Unlike a CPU which is designed to run a relatively low number of threads (dozens), a GPU is designed to run thousands of threads. Even though a single thread performance on CPU is better
than on the GPU, the fact that one can run thousands of threads overcomes the slow performance per thread.


A CUDA device contains an array of _streaming multiprocessors_ (SM).In the CUDA programming model, threads are organized into __thread blocks__. Each __thread blocks__ runs independently on a different SM.

![fig](figs/automatic-scalability.png)
![fig](/img/automatic-scalability.png)




## Example 1

Our first CUDA example doesn't do much but introduces some important syntax
```cpp
__global__ mykernel(){

}
int main(){
    mykernel<<<1,1>>>();
    return 0;
}
```

This is a simple C++ program with the addition of two constructs
1. __global__ keyword. This means the function is called from the host and runs on the device.
1. <<<1,1>>> is a call from host code to device code. Also called a _kernel launch_.

The parameters 1,1 refer to the number of blocks and the number of threads per block, in this case a single thread
in a single block.

## Example 2

This simple example illustrates the typical process for computing with a GPU.

### Processing flow

In what follows the host refers to the CPU/RAM and the device to the GPU. The figure below
shows the processing flow. 

1. Data is transferred from host memory to device memory
1. Computation is done on the device via _kernel_ launch (see later)
1. Data is transferred back from device memory to host memory


![fig](figs/cuda-host-device.png)
![fig](/img/cuda-host-device.png)


```cpp
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <iostream>
__global__ void kernel(int *x,int *y,int *z){
    *z=*x+*y;
}
int main(){
    int a=1,b=2,c=0; //host variables
    int *d_a,*d_b,*d_c;//will hold device addresses
    // allocate memory for one integer and store the
     // address in d_a 
    cudaMalloc(&d_a,sizeof(int));
    cudaMalloc(&d_b,sizeof(int));
    cudaMalloc(&d_c,sizeof(int));
    // copy the value of a and b
    // TO device FROM host
    cudaMemcpy(d_a,&a,sizeof(int),cudaMemcpyHostToDevice);
    cudaMemcpy(d_b,&b,sizeof(int),cudaMemcpyHostToDevice);
    kernel<<<1,1>>>(d_a,d_b,d_c);
    // copy the result TO host FROM device
    cudaMemcpy(&c,d_c,sizeof(int),cudaMemcpyDeviceToHost);
    cudaDeviceSynchronize();
    
std::cout<<"value of c is "<<c<<"\n";
    
}
```

## Example 3

This is the first example where we use parallelism, computing the sum of two arrays. The computation is performed where each thread computes the sum of two elements. To accomplish that we map the thread id to the array index. In this example we use a single, linear, block therefore the thread id is equal to the builtin variable threadIdx.x

```cpp
#include <iostream>
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
__global__ void kernel(float* a, float* b, float* c) {
	int id = threadIdx.x;
	c[id] = a[id] + b[id];
}

int main() {
	int N = 1024;
	float* a, * b, * c;
	float* da, * db, * dc;
  /* allocate memory on host */
	a = (float*)malloc(N * sizeof(float));
	b = (float*)malloc(N * sizeof(float));
	c = (float*)malloc(N * sizeof(float));
  /* allocate memory on device */
	cudaMalloc(&da, N * sizeof(float));
	cudaMalloc(&db, N * sizeof(float));
	cudaMalloc(&dc, N * sizeof(float));
  /* initialize the arrays a and b */
	for (int i = 0; i < N; ++i) {
		a[i] = i;
		b[i] = 2 * i;
	}
  /* copy arrays a and b to device */
	cudaMemcpy(da, a, N * sizeof(float), cudaMemcpyHostToDevice);
	cudaMemcpy(db, b, N * sizeof(float), cudaMemcpyHostToDevice);
/* launch kernel with one block of N threads */
	kernel << <1, N >> > (da, db, dc);
  /* copy result to host */
	cudaMemcpy(c, dc, N * sizeof(float), cudaMemcpyDeviceToHost);
  /* print the first 10 elements */
	for (int i = 0; i < 10; ++i)
		std::cout << c[i] << ' ';
	std::cout << std::endl;
	/* free memory on host and device */
	free(a);
	free(b);
	free(c);
	cudaFree(db);
	cudaFree(dc);
	cudaFree(da);

}

```


## Thread blocks

In CUDA the maximum number of threads in a block is 1024. What if in the previous example we would like to compute the sum of two vectors with size bigger than 1024? We use multiple blocks. We also need to consider the case when the number of total threads is larger than the array size. When using multiple blocks the thread id can be computed as follows(see figure) id=blockIdx.x*blockDim.x+threadIdx.x
![fig](figs/thread-hierarchy.png)
![fig](/img/thread-hierarchy.png)


We repeat the previous example by using multiple blocks. Note that since the number of threads
per block is fixed, the total number of threads might be bigger than the size of the arrays, hence the if condition.

```cpp
#include <iostream>
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
__global__ void kernel(float* a, float* b, float* c,int n) {
	int id = blockIdx.x*blockDim.x+threadIdx.x;
  if(id<n)
	  c[id] = a[id] + b[id];
}

int main() {
	int N = 2048;
	float* a, * b, * c;
	float* da, * db, * dc;
  /* allocate memory on host */
	a = (float*)malloc(N * sizeof(float));
	b = (float*)malloc(N * sizeof(float));
	c = (float*)malloc(N * sizeof(float));
  /* allocate memory on device */
	cudaMalloc(&da, N * sizeof(float));
	cudaMalloc(&db, N * sizeof(float));
	cudaMalloc(&dc, N * sizeof(float));
  /* initialize the arrays a and b */
	for (int i = 0; i < N; ++i) {
		a[i] = i;
		b[i] = 2 * i;
	}
  /* copy arrays a and b to device */
	cudaMemcpy(da, a, N * sizeof(float), cudaMemcpyHostToDevice);
	cudaMemcpy(db, b, N * sizeof(float), cudaMemcpyHostToDevice);
/* divide the load into num_blocks */
  int num_blocks=N/1024;
	kernel << <num_blocks, 1024>> > (da, db, dc,N);
  /* copy result to host */
	cudaMemcpy(c, dc, N * sizeof(float), cudaMemcpyDeviceToHost);
  /* print the last 10 elements */
	for (int i = 0; i < 10; ++i)
		std::cout << c[2047-i] << ' ';
	std::cout << std::endl;
	/* free memory on host and device */
	free(a);
	free(b);
	free(c);
	cudaFree(db);
	cudaFree(dc);
	cudaFree(da);

}

```


## NVIDIA CPUs Memory Hierarchy

The figure below shows the memory hierarchy for a typical NVIDIA GPU. Scalar variables __without qualifiers__ are  stored in registers, arrays in global/local memory. Variables preceded with ```__shared__``` and ```__constant__``` are stored in shared and constant memory respectively.

What concerns us in this section is that __shared__ memory, being on chip (typically 64KB), is much faster to access than DRAM.

![fig](figs/nvidia-gpu-memory.png)
![fig](/img/nvidia-gpu-memory.png)


The figure above gives a clear picture of the way blocks are handled, each block runs independently on an SM.
An important intrinsic primitive provided by CUDA is the ```__syncthread()``` call. It acts like a barrier for all threads in a block.

We will illustrate the above concepts with an implementation of the ```reduce``` operation. It is easier to visualize the reduce operation as a binary tree as in the example below.

![fig](figs/reduction-tree.png)
![fig](/img/reduction-tree.png)

The computation is done in multiple steps with the stride doubling for each step. Initially the stride is 1. Let ```a``` denote the array shown in the figure then the basic operations is 
```cpp
for(int stride=1;stride<n;stride*=2)
  if(i%2==0)
    a[i]+=a[i+stride]
```
If we map the index ```i``` to the thread id then each thread, for a given step, will do a single computation.
At the end of the process thread with id 0 will compute the final value.

```cpp

#define THREADS_PER_BLOCK 1024
__global__ void reduction1(int* da, int* db) {
	__shared__ int sdata[THREADS_PER_BLOCK];
	unsigned int tid = threadIdx.x;
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	sdata[tid] = da[i];
	__syncthreads();
	// do reduction in shared mem
	
	for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {
		if (tid % (2 * stride) == 0) {
			sdata[tid] += sdata[tid + stride];
		}
		__syncthreads();
	}
	// write result for this block to global mem
	if (tid == 0) db[blockIdx.x] = sdata[0];
	
}

```
Note how the array ```sdata``` is declared, meaning it is stored in the fast shared memory of the block whereas ```da``` is stored in DRAM. Therefore, we transfer ```da``` to shared memory before the computation start. Each thread will copy one element from ```da``` to ```sa```. 

After that we have to make sure that all the transfers are completed before we start the computation, hence the first ```__syncthreads()```.

Similarly, since every step in the algorithm depends on the previous step we call ```__synchread()``` at the end of the each step.

In the above example, the size of shared memory was specified at compile time. We can specify it a run time by adding an extra parameter to the kernel launch call. 

```cpp
__global__ void reduction1(int* da, int* db) {
	extern __shared__ int sdata[];
	...
	...
}
int main(){
  ...
  reduction1<<<grid_size,block_size,shared_mem>>>(da,db);
  ...
}
```

Where ```shared_mem``` is the size in bytes.

### Warp scheduling

When a  block of threads is assigned to an SM it partitions them into 
warps and each warp gets scheduled by a warp scheduler for execution.
From the NVIDIA documentation 
```
A warp executes one common instruction at a time,
 so full efficiency is realized when all 
 32 threads of a warp agree on their execution path. 
 If threads of a warp diverge via a data-dependent conditional branch,
  the warp executes each branch path taken, disabling threads that are not on that path. 
```

Form the above we can conclude that there is a performance hit when consecutive threads executed different instructions. In our example, there is a branch based on the thread id
```cpp
if (tid % (2 * stride) == 0) {
			sdata[tid] += sdata[tid + stride];
		}
```
This means that for each warp one group of threads executes the sum whereas the second group skips it forcing the scheduler to have two divergent paths.

One way to fix this problem is to map the array indices to threads with __consecutive__ ids as shown below

```cpp

__global__ void reduction2(int* da, int* db) {
	__shared__ int sdata[THREADS_PER_BLOCK];
	unsigned int tid = threadIdx.x;
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	sdata[tid] = da[i];
	__syncthreads();
	// do reduction in shared mem

	for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {
		int index = 2 * stride * tid;
		if (index<blockDim.x) {
			sdata[index] += sdata[index + stride];
		}
		__syncthreads();
	}
	// write result for this block to global mem
	if (tid == 0) db[blockIdx.x] = sdata[0];

}

```

## Matrix Multiplication

### Simple implementation

In this section we implement matrix multiplication in a straightforward way. For simplicity, we assume square matrices. We store the matrices in a row major representation. For an NxN matrix element a<sub>i</sub><sub>j</sub>=a\[i*N+j\]


To be able to handle more than 1024 entries we need to use multiple blocks. Since we are using two dimensional objects it is convenient to use the 2-d representation of thread indices provided by CUDA: threadIdx.x,threadIdx.y and blockIdx.x, blockIdx.y.

Each thread will compute __one__ element of the resulting multiplication as shown below.

```cpp
_global__ void mat_mult(float* da, float* db, float* dc, int width) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    float result = 0;
    for (int k = 0; k < width; ++k) 
    {
        result += da[row * width + k] * db[k * width + col];
    }
    dc[row * width + col] = result;
}
```
We call ```mat_mult``` indirectly via ```time_kernel```.

```cpp
void time_kernel(float* da, float* db, float* dc, int width,
                         dim3 blocks_per_grid,dim3 threads_per_block) {
    cudaEvent_t kernel_start, kernel_end;
    cudaEventCreate(&kernel_start);
    cudaEventCreate(&kernel_end);
    /* warmup call*/
    mat_mult <<<blocks_per_grid, threads_per_block >> > (da, db, dc, width);
    float time = 0;
    float total = 0;
   
    for (int i = 0; i < 100; ++i) {
        cudaEventRecord(kernel_start);
        mat_mult << <blocks_per_grid, threads_per_block>> > (da, db, dc, width);
        cudaEventRecord(kernel_end);
        cudaEventSynchronize(kernel_end);
        cudaEventElapsedTime(&time, kernel_start, kernel_end);
        total += time;
    }
    /* average time in milliseconds */
    std::cout << "time " << total / 100 << '\n';

}

```


### Matrix Computation 2

To take advantage of shared memory we divide the matrices in tiles as shown in the figure below. Each block in the grid will compute one tile of matrix C (yellow). This is done by dividing the computation as a sequence of computations on the tiles in A and B. In the figure below the result (yellow) is the sum of two computations over tiles: read followed by blue. We need to load the corresponding tiles of A and B before the computation starts. The strategy in the example below is as follows
1. load the red tiles into shared memory
1. compute the product of reds
1. load the blue tiles into shared memory
1. add the computation of the product to the previous result

Loading of tiles is done in parallel. Each thread of the block will load one element of A and one element of B.

The above strategy looks promising but it has a flaw. A thread might start computation __before__ all threads have loaded their respective elements. Also, we need to make sure that subsequent loads do not start until all threads have finished computing with the current data.
We have seen this kind of synchronization primitive before, it is called a __barrier__. In CUDA, the ```arrive_and_wait()``` functionality of a barrier is performed with the ```__syncthreads()``` functions. Note that it synchronizes threads in the __same block__ only.
![fig](figs/matrix.png)
![fig](/img/matrix.png)


```cpp

#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <iostream>
#include <algorithm>


#define BLOCK_SIZE 32
__global__ void mult(float* da, float* db, float* dc, int width) {

	int by= blockIdx.y;
	int bx = blockIdx.x;
	int ty = threadIdx.y;
	int tx = threadIdx.x;
	int row = by * BLOCK_SIZE + ty;
	int col = bx * BLOCK_SIZE + tx;
	__shared__ float sa[BLOCK_SIZE][BLOCK_SIZE];
	__shared__ float sb[BLOCK_SIZE][BLOCK_SIZE];
	float res = 0.0;
	int ntiles = width / BLOCK_SIZE;
	for (int b = 0; b < ntiles; ++b) {
		
		/* copy from memory to shared memory */
		sa[ty][tx] = da[row * width + b * BLOCK_SIZE + tx];
		sb[ty][tx] = db[(b * BLOCK_SIZE + ty) * width + col];
		
		__syncthreads();
		for (int k = 0; k < BLOCK_SIZE; ++k) {
			res += sa[ty][k] * sb[k][tx];
		}
		__syncthreads();
	}
	dc[row* width + col] = res;
}


int main() {
	cudaEvent_t kernel_start,kernel_end;
	cudaEventCreate(&kernel_start);
	cudaEventCreate(&kernel_end);


	float* a, * b, * c;
	float* da, * db, * dc;

	const int matrix_width = 1024;
	const int size = matrix_width * matrix_width;
	a = (float*)malloc(size * sizeof(float));
	b = (float*)malloc(size * sizeof(float));
	c = (float*)malloc(size * sizeof(float));
	for (int i = 0; i < size; ++i) {
		a[i] = 1;
		b[i] = 1;
	}
	cudaMalloc(&da, size * sizeof(float));
	cudaMalloc(&db, size * sizeof(float));
	cudaMalloc(&dc, size * sizeof(float));
	cudaMemcpy(da, a, size * sizeof(float), cudaMemcpyHostToDevice);
	cudaMemcpy(db, b, size * sizeof(float), cudaMemcpyHostToDevice);
	dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);
	dim3 gridSize(matrix_width/ BLOCK_SIZE, matrix_width / BLOCK_SIZE);
	mult << <gridSize, blockSize >> > (da, db, dc, matrix_width);
	float time = 0;
	float total = 0;

	for (int i = 0; i < 500; ++i) {
		cudaEventRecord(kernel_start,0);
		mult << <gridSize, blockSize >> > (da, db, dc, matrix_width);
		cudaEventRecord(kernel_end,0);
		cudaEventSynchronize(kernel_end);
		cudaEventElapsedTime(&time, kernel_start, kernel_end);
		total += time;
	}
	std::cout << "average time " << total/500 << '\n';
	cudaMemcpy(c, dc, size * sizeof(float), cudaMemcpyDeviceToHost);
	/*for (int i = 0; i < 64; i++)
		std::cout << c[i] << ' ';*/
	std::cout << std::endl;
	cudaFree(da);
	cudaFree(db);
	cudaFree(dc);
	free(a);
	free(b);
	free(c);

}

```

## Exclusive Scan

![fig](figs/ble-scan-fig1.png)
![fig](/img/ble-scan-fig1.png)



![fig](figs/ble-scan-fig2.png)
![fig](/img/ble-scan-fig2.png)



By looking at the figure it is clear that the value at node \\(A\\) is the sum of the values of its children.
The index of node \\(A\\) is at \\(2^{h+1}-1\\) which is the same as its right child. The left child of ```A``` has index \\(2^h-1\\). Let ```S``` be the array then 

  $$  S[2^{h+1}-1]=S[2^h-1]+S[2^{h+1}-1]$$

For node ```B``` it is the same except we shift the indices by \\(2^{h+1}\\) to the right.
Therefore for nodes at level ```h+1``` we have 

   $$ S[k+2^{h+1}-1]=S[k+2^h-1]+S[k+2^{h+1}-1]$$

where ```k``` starts with 0 and is incremented by \\(2^{h+1}\\).

We perform the above for every level ```h```.

![fig](figs/ble-scan.png)
![fig](/img/ble-scan.png)

Note that in the last algorithm alpha is at most ```n/2-1``` which means we need half the number of threads.